# -*- coding: utf-8 -*-
"""data_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Enl37WUrzT7PFp0nMSPfwtgmpxcn2t8
"""

from google.colab import drive
drive.mount('/content/drive')

"""# CS506 - Team 1 Project """

import pandas as pd
import numpy  as np
import matplotlib.pyplot as plt # we can also use seaborn for prettier graphs or bokeh for interactive graphs 
import statistics as stat

"""## Data preprocessing"""

DATASET_PATH = '/content/drive/Shareddrives/CS506_Team/Data/Blake_RPD_Dataset-2.xlsx'
df = pd.read_excel(DATASET_PATH, "data") # data is the sheet name
df

# gender: male = 0. female = 1
# location: remote = 0. in classrom = 1
# cooperation = 0. defection = 1

# tfti = Decision made in ith round when facing Tit for Tat Partner.
# TFT partner cooperates on the 1st round and then repeats the child's decision in following rounds

# coopi = Decision made in ith round when facing cooperative Partner.
# Coop partner cooperates on all rounds BUT defects on rounds 3 & 7

# defi = Decision made in ith round when facing the defective partner.
# Def partner defects on all rounds BUT cooperates on rounds 3 & 7.

# Here, I generate list of column names for easy access:

tft = ['tft' + str(i) for i in range(1,11)]
tft_rt = ['tft_rt' + str(i) for i in range(1,11)]
coop = ['coop' + str(i) for i in range(1,11)]
coop_rt = ['coop_rt' + str(i) for i in range(1,11)]
defs = ['def' + str(i) for i in range(1,11)]
def_rt = ['def_rt' + str(i) for i in range(1,11)]

features = [tft, tft_rt, coop, coop_rt, defs, def_rt]

# Now you can simply call the data frame + feature name. Try it!
df[tft]

#Dropping rows with NaN values:

dropped_len = df.shape[0] - df.dropna().shape[0] 
data = df.dropna() #Now, we have a clean data frame. 

print('Number of datapoints dropped:', dropped_len)

# Calculating averages - feel free to manipulate 
# these dataframes however way you see fit
df['tft_avg'] = df[tft].mean(axis=1)
df['tft_rt_avg'] = df[tft_rt].mean(axis=1)
df['coop_avg'] = df[coop].mean(axis=1)
df['coop_rt_avg'] = df[coop_rt].mean(axis=1)
df['def_avg'] = df[defs].mean(axis=1)
df['def_rt_avg'] = df[def_rt].mean(axis=1)

avgs = ['tft_avg','tft_rt_avg','coop_avg','coop_rt_avg','def_avg','def_rt_avg']
df[avgs]

# a row of column means
col_means = df.mean()
col_means

# Whether children are naturally cooperative/defiant as their parents reported
def scatter_plt(x,y,xlabel,ylabel,title):
  plt.scatter(x,y)
  plt.xlabel(xlabel)
  plt.ylabel(ylabel)
  plt.title(title)
  plt.show()
  plt.close()

proactive_aggr = list(df['P-Proactive_aggr'])
reactive_aggr = list(df['P-Reactive_aggr'])
tft_avg = list(df['tft_avg'])
coop_avg = list(df['coop_avg'])
def_avg = list(df['def_avg'])
scatter_plt(proactive_aggr,tft_avg,'reported aggr','avg decision','Average Decision vs Proactive Aggr Score When Playing Against TFT Partner')
scatter_plt(proactive_aggr,coop_avg,'reported aggr','avg decision','Average Decision vs Proactive Aggr Score When Playing Against Coop Partner')
scatter_plt(proactive_aggr,def_avg,'reported aggr','avg decision','Average Decision vs Proactive Aggr Score When Playing Against Def Partner')
scatter_plt(reactive_aggr,tft_avg,'reported aggr','avg decision','Average Decision vs Reactive Aggr Score When Playing Against TFT Partner')
scatter_plt(reactive_aggr,coop_avg,'reported aggr','avg decision','Average Decision vs Reactive Aggr Score When Playing Against Coop Partner')
scatter_plt(reactive_aggr,def_avg,'reported aggr','avg decision','Average Decision vs Reactive Aggr Score When Playing Against Def Partner')

def Aggr_DecAvg_Plot(reported_aggr,dec_avg,title):
  l = [(0,0)] * 20
  for child in range(len(reported_aggr)):
    if reported_aggr[child] != reported_aggr[child]:
      continue
    i = int(reported_aggr[child])
    sum_dec,count = l[i]
    sum_dec += dec_avg[child]
    count += 1
    l[i] = (sum_dec,count)
  aggr_dec_avg = []
  for i in range(len(l)):
    sum_dec,count = l[i]
    if count == 0:
      break;
    aggr_dec_avg.append(sum_dec/count)
  aggr_scores = range(len(aggr_dec_avg))
  plt.plot(aggr_scores,aggr_dec_avg)
  plt.xlabel('Aggr Score')
  plt.ylabel('Avg Decision')
  plt.title(title)
  plt.show()
  plt.close

Aggr_DecAvg_Plot(proactive_aggr,tft_avg,'Reported Proactive Aggr Score vs Average Decision of All Children against TFT Partner')
Aggr_DecAvg_Plot(proactive_aggr,coop_avg,'Reported Proactive Aggr Score vs Average Decision of All Children against Coop Partner')
Aggr_DecAvg_Plot(proactive_aggr,def_avg,'Reported Proactive Aggr Score vs Average Decision of All Children against Def Partner')
Aggr_DecAvg_Plot(reactive_aggr,tft_avg,'Reported Reactive Aggr Score vs Average Decision of All Children against TFT Partner')
Aggr_DecAvg_Plot(reactive_aggr,coop_avg,'Reported Reactive Aggr Score vs Average Decision of All Children against Coop Partner')
Aggr_DecAvg_Plot(reactive_aggr,def_avg,'Reported Reactive Aggr Score vs Average Decision of All Children against Def Partner')

# Change in children after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = [df['coop' + str(x)].mean() for x in range(3,6)]
plt.plot([3,4,5],y,color='blue',linewidth=2,label="Cooperative")
y = [df['coop' + str(x)].mean() for x in range(7,10)]
plt.plot([7,8,9],y,color='blue',linewidth=2)
y = [df['def' + str(x)].mean() for x in range(3,6)]
plt.plot([3,4,5],y,color='red',linewidth=2,label="Defiant")
y = [df['def' + str(x)].mean() for x in range(7,10)]
plt.plot([7,8,9],y,color='red',linewidth=2)
plt.legend()
plt.title('Average Responses After Deviation in Opponent')
plt.show()

# Change in children with more reactive aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = []
helper = []
helper_mean = 0
for i in df['P-Reactive_aggr'].iteritems():
  if i[1] >= 2:
    helper.append(i[0])
for x in range(3,6):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([3,4,5],y,color='blue',linewidth=2,label="Cooperative")
y = []
for x in range(7,10):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([7,8,9],y,color='blue',linewidth=2)
y = []
for x in range(3,6):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([3,4,5],y,color='red',linewidth=2,label="Defiant")
y = []
for x in range(7,10):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([7,8,9],y,color='red',linewidth=2)
plt.legend()
plt.title('Average Responses for Children with More Reactive Aggression After Deviation in Opponent')
plt.show()

# Change in children with more proactive aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = []
helper = []
helper_mean = 0
for i in df['P-Proactive_aggr'].iteritems():
  if i[1] >= .5:
    helper.append(i[0])
for x in range(3,6):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([3,4,5],y,color='blue',linewidth=2,label="Cooperative")
y = []
for x in range(7,10):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([7,8,9],y,color='blue',linewidth=2)
y = []
for x in range(3,6):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([3,4,5],y,color='red',linewidth=2,label="Defiant")
y = []
for x in range(7,10):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([7,8,9],y,color='red',linewidth=2)
plt.legend()
plt.title('Average Responses for Children with More Proactive Aggression After Deviation in Opponent')
plt.show()

# Change in children with more total aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = []
helper = []
helper_mean = 0
for i in df['P-Aggression_Total'].iteritems():
  if i[1] >= 3:
    helper.append(i[0])
for x in range(3,6):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([3,4,5],y,color='blue',linewidth=2,label="Cooperative")
y = []
for x in range(7,10):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([7,8,9],y,color='blue',linewidth=2)
y = []
for x in range(3,6):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([3,4,5],y,color='red',linewidth=2,label="Defiant")
y = []
for x in range(7,10):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  y.append(helper_mean/len(helper))
  helper_mean = 0
plt.plot([7,8,9],y,color='red',linewidth=2)
plt.legend()
plt.title('Average Responses for Children with More Total Aggression After Deviation in Opponent')
plt.show()

np.array(data['P-Proactive_aggr'])

#Takes processed dataframe and isolates reaction times and game choices for a single participant at a specific index point.
def isolate_participant(df, patient_index): 
  patient_df=df.iloc[patient_index]
  return patient_df[tft], patient_df[tft_rt]
[actions, reaction_times]=isolate_participant(df, 0)

#Determines state based on differences in reaction times from a starting point. The eval point is used to determine the participant's state after a given round. 
def determine_state(reaction_times, actions, eval_point):
    defect_times=[]
    defect_rounds=[]
    cooperate_times=[]
    cooperate_rounds=[]
    #At the moment, there is no consideration for a "balanced state" where reaction times for cooperation and defection are roughly equal. 
    #Discussion about what constitutes distinctively different reaction times to come.
    for i, (action, time) in enumerate(zip(actions, reaction_times)):
      if i >= eval_point:
        if action == 1:
          defect_times.append(time)
          defect_rounds.append(i)
        if action ==0:
          cooperate_times.append(time)
          cooperate_rounds.append(i)
    assert len(cooperate_rounds)>0, "Data must contain at least one cooperative round"
    assert len(defect_rounds)>0, "Data must contain at least one uncooperative round" 
    print('Cooperative reaction times: ', cooperate_times)
    print('Defection reaction times: ', defect_times)
    plt.scatter(defect_rounds, defect_times, c = 'red', label='defection')
    plt.scatter(cooperate_rounds, cooperate_times, c = 'blue', label='cooperation')
    plt.title("Reaction Times for Each Round")
    plt.xlabel('Round Number')
    plt.ylabel('Reaction Time (seconds)')
    plt.legend()
    plt.show()
    if stat.mean(defect_times) > stat.mean(cooperate_times):
      return 'cooperative'
    if stat.mean(cooperate_times) > stat.mean(defect_times):
      return 'uncooperative'
    else:
      return 'Could not determine state'

print(determine_state(reaction_times, actions, 0))

"""
A preliminary determination of a participant’s state was constructed using the difference in reaction times between cooperation and defection choices in subsequent rounds. 
In addition to raw cooperation and defection data, this model will be useful in investigating the long-term change in a participant’s preference over many rounds. 
The preliminary model assesses relative reaction times after a round specified by the user. In the future, this function will take into account the overall trends in reaction time. 
Additionally, the model does not account for roughly equal cooperative and uncooperative reaction times. 
What constitutes roughly equal reaction times will need to be discussed with Peter. There seems to be significant differences between cooperative and uncooperative reaction times for many participants. 
A formal drift diffusion model will eventually be required for a rigorous analysis of the primary research questions covered in this project. 

"""

# Change in children with more reactive aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Reactive_aggr'].iteritems():
  if i[1] >= 3:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  for i in helper1:
    helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Reactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Reactively Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  for i in helper1:
    helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Reactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Reactively Agressive")

plt.legend()
plt.title('Average Responses for Children By Reactive Aggression')
plt.show()

# Change in children with more proactive aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Proactive_aggr'].iteritems():
  if i[1] >= 1:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  for i in helper1:
    helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Proactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Proactively Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  for i in helper1:
    helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0
plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Proactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Proactively Agressive")

plt.legend()
plt.title('Average Responses for Children By Proactive Aggression')
plt.show()

# Change in children with more total aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Response', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Aggression_Total'].iteritems():
  if i[1] >= 1:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  for i in helper1:
    helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def' + str(x)].values
  for i in helper:
    helper_mean += values[i]
  for i in helper1:
    helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0
plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Agressive")
plt.legend()
plt.title('Average Responses for Children By Total Aggression')
plt.show()

# Similar to above, but will focus on visualizing reaction time
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Reaction Time', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Reactive_aggr'].iteritems():
  if i[1] >= 3:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop_rt' + str(x)].values
  coop = df['coop' + str(x)].values
  for i in helper:
    if int(coop[i]) == 1:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 1:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Reactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Reactively Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def_rt' + str(x)].values
  coop = df['def' + str(x)].values
  for i in helper:
    if int(coop[i]) == 1:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 1:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Reactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Reactively Agressive")

plt.legend()
plt.title('Average Reaction Times for Children Who Cooperated By Reactive Aggression')
plt.show()


plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Reaction Time', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Reactive_aggr'].iteritems():
  if i[1] >= 3:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop_rt' + str(x)].values
  coop = df['coop' + str(x)].values
  for i in helper:
    if int(coop[i]) == 0:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 0:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Reactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Reactively Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def_rt' + str(x)].values
  coop = df['def' + str(x)].values
  for i in helper:
    if int(coop[i]) == 0:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 0:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Reactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Reactively Agressive")

plt.legend()
plt.title('Average Reaction Times for Children Who Defected By Reactive Aggression')
plt.show()




# Change in children with more proactive aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Reaction Time', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Proactive_aggr'].iteritems():
  if i[1] >= 1:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop_rt' + str(x)].values
  coop = df['coop' + str(x)].values
  for i in helper:
    if int(coop[i]) == 1:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 1:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Proactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Proactively Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def_rt' + str(x)].values
  coop = df['def' + str(x)].values
  for i in helper:
    if int(coop[i]) == 1:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 1:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0
plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Proactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Proactively Agressive")

plt.legend()
plt.title('Average Reaction Times for Children Who Cooperated By Proactive Aggression')
plt.show()

plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Reaction Time', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Proactive_aggr'].iteritems():
  if i[1] >= 1:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop_rt' + str(x)].values
  coop = df['coop' + str(x)].values
  for i in helper:
    if int(coop[i]) == 0:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 0:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Proactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Proactively Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def_rt' + str(x)].values
  coop = df['def' + str(x)].values
  for i in helper:
    if int(coop[i]) == 0:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 0:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0
plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Proactively Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Proactively Agressive")

plt.legend()
plt.title('Average Reaction Times for Children Who Defected By Proactive Aggression')
plt.show()



# Change in children with more total aggression after deviation
plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Reaction Time', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Aggression_Total'].iteritems():
  if i[1] >= 1:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop_rt' + str(x)].values
  coop = df['coop' + str(x)].values
  for i in helper:
    if int(coop[i]) == 1:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 1:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def_rt' + str(x)].values
  coop = df['def' + str(x)].values
  for i in helper:
    if int(coop[i]) == 1:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 1:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0
plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Agressive")
plt.legend()
plt.title('Average Reaction Times for Children Who Cooperated By Total Aggression')
plt.show()



plt.figure(num=None,figsize=(7, 5))
plt.xlabel('Trial Number', fontsize=14)
plt.ylabel('Average Reaction Time', fontsize=14)
y = []
y1= []
helper = []
helper1 = []
helper_mean = 0
helper1_mean = 0
for i in df['P-Aggression_Total'].iteritems():
  if i[1] >= 1:
    helper.append(i[0])
  else:
    helper1.append(i[0])
for x in range(1,11):
  values = df['coop_rt' + str(x)].values
  coop = df['coop' + str(x)].values
  for i in helper:
    if int(coop[i]) == 0:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 0:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0

plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='blue',linewidth=2,label="Cooperative - More Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='green',linewidth=2,label="Cooperative - Less Agressive")
y = []
y1 = []
for x in range(1,11):
  values = df['def_rt' + str(x)].values
  coop = df['def' + str(x)].values
  for i in helper:
    if int(coop[i]) == 0:
      helper_mean += values[i]
  for i in helper1:
    if int(coop[i]) == 0:
      helper1_mean += values[i]
  y.append(helper_mean/len(helper))
  y1.append(helper1_mean/len(helper1))
  helper_mean = 0
  helper1_mean = 0
plt.plot([1,2,3,4,5,6,7,8,9,10],y,color='red',linewidth=2,label="Defiant - More Agressive")
plt.plot([1,2,3,4,5,6,7,8,9,10],y1,color='yellow',linewidth=2,label="Defiant - Less Agressive")
plt.legend()
plt.title('Average Reaction Times for Children Who Defected By Total Aggression')
plt.show()

"""## Logistic Regression Model
For predicting the kids' aggression scores when trained using their decisions against each kind of partners respectively.

by Jessica Chen on Mar 18, 2021
"""

import numpy as np

"""
Converts an array of aggression scores into an array of
0 and 1's. 0 meaning score below median, and 1 meaning score
above median.
We are converting the scores like this because some score groups
are so underpopulated such that they only have 1 member """
def convert_by_median(aggr):
  """
  :param aggr: an array of a particular aggression score
  """
  med = np.median(aggr)
  for i in range(aggr.size):
    if aggr[i]<med:
      aggr[i] = 0
    else:
      aggr[i] = 1
  return aggr

"""
A helper function for preparing an array of aggression scores
before training the Logictic Regression Model.
We convert the datafram column to numpy array, finds the nan
indices, drop the nan's, and convert the array into 0 and 1's
by its median.
Returns the processed array and nan indices. """
def get_aggr(aggr_label):
  aggr = df[aggr_label].to_numpy()
  aggr_na = np.argwhere(np.isnan(aggr))
  aggr = aggr[~np.isnan(aggr)]
  aggr = convert_by_median(np.nan_to_num(aggr))
  return aggr, aggr_na

"""
Perform 5-fold cross validation experiments on the Logistic
Regression Model trained using the given kids' decisions and
their aggression scores.
Returns the average accuracy of the model"""
def LR_5fold_crossvalid(kids,aggr):
  """"
  :param kids: the kids' decisions against a particular opponent type
  :param aggr: array of a particular aggression score
  """
  from sklearn.model_selection import StratifiedKFold
  from sklearn.linear_model import LogisticRegression
  skf = StratifiedKFold()  #n_splits = 5 by default
  acc = [] #list of accuracy scores

  for train_index, test_index in skf.split(kids,aggr):
    train_kids = kids[train_index]
    train_aggr = aggr[train_index]
    test_kids = kids[test_index]
    test_aggr = aggr[test_index]
    
    clf = LogisticRegression().fit(train_kids, train_aggr)
    acc.append(clf.score(test_kids, test_aggr))
  return sum(acc)/len(acc)

proactive_aggr, proactive_na = get_aggr('P-Proactive_aggr')
reactive_aggr,reactive_na = get_aggr('P-Reactive_aggr')
tot_aggr, tot_na = get_aggr('P-Aggression_Total')


"""All 10 Rounds---------------------------------------------"""
kids_tft = df[tft].to_numpy()
kids_def = df[defs].to_numpy()
print(df[coop])
kids_coop = df[coop].to_numpy()

print("The average accuracy of Logistic Regression Model when trained\n",
"using all 10 of kids' decisions is:")

print("\n1. against TFT opponent: ")
acc_tft_proactive = LR_5fold_crossvalid(np.delete(kids_tft, proactive_na,axis=0),proactive_aggr)
print("- ", acc_tft_proactive, "in predicting the PROACTIVE aggression scores")
acc_tft_reactive = LR_5fold_crossvalid(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print("- ", acc_tft_reactive, "in predicting the REACTIVE aggression scores")
acc_tft_tot = LR_5fold_crossvalid(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print("- ", acc_tft_reactive, "in predicting the TOTAL aggression scores")

print("\n2. against COOP opponent: ")
acc_coop_proactive = LR_5fold_crossvalid(np.delete(kids_coop, proactive_na,axis=0),proactive_aggr)
print("- ", acc_coop_proactive, "in predicting the PROACTIVE aggression scores")
acc_coop_reactive = LR_5fold_crossvalid(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print("- ", acc_coop_reactive, "in predicting the REACTIVE aggression scores")
acc_coop_tot = LR_5fold_crossvalid(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print("- ", acc_coop_reactive, "in predicting the TOTAL aggression scores")

print("\n3. against DEFECTIVE opponent: ")
acc_def_proactive = LR_5fold_crossvalid(np.delete(kids_def, proactive_na,axis=0),proactive_aggr)
print("- ", acc_def_proactive, "in predicting the PROACTIVE aggression scores")
acc_def_reactive = LR_5fold_crossvalid(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print("- ", acc_def_reactive, "in predicting the REACTIVE aggression scores")
acc_def_tot = LR_5fold_crossvalid(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print("- ", acc_def_reactive, "in predicting the TOTAL aggression scores")


"""Just the later 7 rounds----------------------------------------------"""
kids_tft = df[['tft' + str(i) for i in range(4,11)]].to_numpy()
kids_def = df[['def' + str(i) for i in range(4,11)]].to_numpy()
kids_coop = df[['coop' + str(i) for i in range(4,11)]].to_numpy()

print("\n\n\nThe average accuracy of Logistic Regression Model when trained\n",
"using the kids' decisions on rounds 4 to 10 is:")

print("\n1. against TFT opponent: ")
acc_tft_proactive = LR_5fold_crossvalid(np.delete(kids_tft, proactive_na,axis=0),proactive_aggr)
print("- ", acc_tft_proactive, "in predicting the PROACTIVE aggression scores")
acc_tft_reactive = LR_5fold_crossvalid(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print("- ", acc_tft_reactive, "in predicting the REACTIVE aggression scores")
acc_tft_tot = LR_5fold_crossvalid(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print("- ", acc_tft_reactive, "in predicting the TOTAL aggression scores")

print("\n2. against COOP opponent: ")
acc_coop_proactive = LR_5fold_crossvalid(np.delete(kids_coop, proactive_na,axis=0),proactive_aggr)
print("- ", acc_coop_proactive, "in predicting the PROACTIVE aggression scores")
acc_coop_reactive = LR_5fold_crossvalid(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print("- ", acc_coop_reactive, "in predicting the REACTIVE aggression scores")
acc_coop_tot = LR_5fold_crossvalid(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print("- ", acc_coop_reactive, "in predicting the TOTAL aggression scores")

print("\n3. against DEFECTIVE opponent: ")
acc_def_proactive = LR_5fold_crossvalid(np.delete(kids_def, proactive_na,axis=0),proactive_aggr)
print("- ", acc_def_proactive, "in predicting the PROACTIVE aggression scores")
acc_def_reactive = LR_5fold_crossvalid(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print("- ", acc_def_reactive, "in predicting the REACTIVE aggression scores")
acc_def_tot = LR_5fold_crossvalid(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print("- ", acc_def_reactive, "in predicting the TOTAL aggression scores")


"""Just 4th and 8th rounds----------------------------------------------"""
kids_def = df[['def' + str(i) for i in [4,8]]].to_numpy()
kids_coop = df[['coop' + str(i) for i in [4,8]]].to_numpy()

print("\n\n\nThe average accuracy of Logistic Regression Model when trained\n",
"using the kids' decisions on rounds 4 and 8 is:")

print("\n2. against COOP opponent: ")
acc_coop_proactive = LR_5fold_crossvalid(np.delete(kids_coop, proactive_na,axis=0),proactive_aggr)
print("- ", acc_coop_proactive, "in predicting the PROACTIVE aggression scores")
acc_coop_reactive = LR_5fold_crossvalid(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print("- ", acc_coop_reactive, "in predicting the REACTIVE aggression scores")
acc_coop_tot = LR_5fold_crossvalid(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print("- ", acc_coop_reactive, "in predicting the TOTAL aggression scores")

print("\n3. against DEFECTIVE opponent: ")
acc_def_proactive = LR_5fold_crossvalid(np.delete(kids_def, proactive_na,axis=0),proactive_aggr)
print("- ", acc_def_proactive, "in predicting the PROACTIVE aggression scores")
acc_def_reactive = LR_5fold_crossvalid(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print("- ", acc_def_reactive, "in predicting the REACTIVE aggression scores")
acc_def_tot = LR_5fold_crossvalid(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print("- ", acc_def_reactive, "in predicting the TOTAL aggression scores")

"""## Continuous Model
Exploration of continuous prediction with multiple ML algorithms.
Random forest regression, support vector regression, and linear regression

Brendan Leap


"""

import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
def get_aggr(aggr_label):
  aggr = df[aggr_label].to_numpy()
  aggr_na = np.argwhere(np.isnan(aggr))
  aggr = aggr[~np.isnan(aggr)]
  return aggr, aggr_na

def continuous_variable_modeling(kids, aggr):


  train_kids, test_kids, train_aggr, test_aggr = train_test_split(kids, aggr, test_size=.25)
    
  rfr = RandomForestRegressor().fit(train_kids, train_aggr)
  rfr_pred=rfr.predict(test_kids)
  r_squared_rfr=rfr.score(test_kids, test_aggr)
  rmse_rfr=mean_squared_error(rfr_pred, test_aggr)
  #print("- R squared:", r_squared_rfr, " for Random Forest Regression")
  print("-     RMSE:", rmse_rfr, " for Random Forest Regression")
  #print(rfr_pred)
  #print(test_aggr)

  svr = SVR().fit(train_kids, train_aggr)
  svr_pred=svr.predict(test_kids)
  r_squared_svr=svr.score(test_kids, test_aggr)
  rmse_svr=mean_squared_error(svr_pred, test_aggr)
  #print("- R squared:", r_squared_svr, "for Support Vector Regression")
  print("-     RMSE:", rmse_svr, "for Support Vector Regression")
  #print(svr_pred)
  #print(test_aggr)

  lr = LinearRegression().fit(train_kids, train_aggr)
  lr_pred=lr.predict(test_kids)
  r_squared_lr=lr.score(test_kids, test_aggr)
  rmse_lr=mean_squared_error(lr_pred, test_aggr)
  #print("- R squared:", r_squared_lr, " for Linear Regression")
  print("-     RMSE:", rmse_lr, " for Linear Regression")
  #print(lr_pred)
  #print(test_aggr)

proactive_aggr, proactive_na = get_aggr('P-Proactive_aggr')
reactive_aggr,reactive_na = get_aggr('P-Reactive_aggr')
tot_aggr, tot_na = get_aggr('P-Aggression_Total')


"""All 10 Rounds---------------------------------------------"""
kids_tft = df[tft].to_numpy()
kids_def = df[defs].to_numpy()
kids_coop = df[coop].to_numpy()

print("The average accuracy of the continuous variable models when trained\n",
"using all 10 of kids' decisions is:")
print("\n1. against TFT opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_tft, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_tft, tot_na,axis=0),tot_aggr)


print("\n2. against COOP opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_coop, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_coop, tot_na,axis=0),tot_aggr)

print("\n3. against DEFECTIVE opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_def, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_def, tot_na,axis=0),tot_aggr)

"""Just the later 7 rounds----------------------------------------------"""
kids_tft = df[['tft' + str(i) for i in range(4,11)]].to_numpy()
kids_def = df[['def' + str(i) for i in range(4,11)]].to_numpy()
kids_coop = df[['coop' + str(i) for i in range(4,11)]].to_numpy()

print("\n\n\nThe average accuracy of continuous variable models when trained\n",
"using the kids' decisions on rounds 4 to 10 is:")
print("\n1. against TFT opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_tft, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_tft, tot_na,axis=0),tot_aggr)


print("\n2. against COOP opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_coop, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_coop, tot_na,axis=0),tot_aggr)

print("\n3. against DEFECTIVE opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_def, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_def, tot_na,axis=0),tot_aggr)

"""Just 4th and 8th rounds----------------------------------------------"""
kids_def = df[['def' + str(i) for i in [4,8]]].to_numpy()
kids_coop = df[['coop' + str(i) for i in [4,8]]].to_numpy()

print("\n\n\nThe average accuracy of continuous variable models when trained\n",
"using the kids' decisions on rounds 4 and 8 is:")

print("\n1. against TFT opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_tft,proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_tft, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_tft, tot_na,axis=0),tot_aggr)


print("\n2. against COOP opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_coop, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_coop, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_coop, tot_na,axis=0),tot_aggr)

print("\n3. against DEFECTIVE opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_def, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_def, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_def, tot_na,axis=0),tot_aggr)

"""##Implementing models for Reaction Time(Brendon's continuous_variable_modeling function can modified to make a generic continuous variable aggression prediction function which can be used for both responses and reaction time)
This function(continuous_variable_aggression_pred) can be deleted later during code cleaning.
"""

import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
def get_aggr(aggr_label):
  aggr = df[aggr_label].to_numpy()
  aggr_na = np.argwhere(np.isnan(aggr))
  aggr = aggr[~np.isnan(aggr)]
  return aggr, aggr_na

def continuous_variable_aggression_pred(kids_rt, aggr):


  train_kids_rt, test_kids_rt, train_aggr, test_aggr = train_test_split(kids_rt, aggr, test_size=.20)
    
  rfr = RandomForestRegressor().fit(train_kids_rt, train_aggr)
  rfr_pred=rfr.predict(test_kids_rt)
  rfr_pred=[int(round(x)) for x in rfr_pred]
  r_squared_rfr=rfr.score(test_kids_rt, test_aggr)
  rmse_rfr=mean_squared_error(rfr_pred, test_aggr)
  # print("- R squared:", r_squared_rfr, " for Random Forest Regression")
  print("-     RMSE:", rmse_rfr, " for Random Forest Regression")


  svr = SVR().fit(train_kids_rt, train_aggr)
  svr_pred=svr.predict(test_kids_rt)
  svr_pred=[int(round(x)) for x in svr_pred]
  r_squared_svr=svr.score(test_kids_rt, test_aggr)
  rmse_svr=mean_squared_error(svr_pred, test_aggr)
  print("-     RMSE:", rmse_svr, "for Support Vector Regression")
  # print("- R squared:", r_squared_svr, "for Support Vector Regression")


  lr = LinearRegression().fit(train_kids_rt, train_aggr)
  lr_pred=lr.predict(test_kids_rt)
  lr_pred=[int(round(x)) for x in lr_pred]
  r_squared_lr=lr.score(test_kids_rt, test_aggr)
  rmse_lr=mean_squared_error(lr_pred, test_aggr)
  print("-     RMSE:", rmse_lr, " for Linear Regression")
  # print("- R squared:", r_squared_lr, " for Linear Regression")


proactive_aggr, proactive_na = get_aggr('P-Proactive_aggr')
reactive_aggr,reactive_na = get_aggr('P-Reactive_aggr')
tot_aggr, tot_na = get_aggr('P-Aggression_Total')

"""All 10 Rounds---------------------------------------------"""
kids_tft_rt = df[tft_rt].to_numpy()
kids_def_rt = df[def_rt].to_numpy()
kids_coop_rt = df[coop_rt].to_numpy()

print("The average accuracy of the continuous variable models when trained\n",
"using all 10 of kids' reaction time is:")
print("\n1. against TFT opponent: ")
print('Proactive Agression')
continuous_variable_aggression_pred(np.delete(kids_tft_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_aggression_pred(np.delete(kids_tft_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_aggression_pred(np.delete(kids_tft_rt, tot_na,axis=0),tot_aggr)


print("\n2. against COOP opponent: ")
print('Proactive Agression')
continuous_variable_aggression_pred(np.delete(kids_coop_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_aggression_pred(np.delete(kids_coop_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_aggression_pred(np.delete(kids_coop_rt, tot_na,axis=0),tot_aggr)

print("\n3. against DEFECTIVE opponent: ")
print('Proactive Agression')
continuous_variable_aggression_pred(np.delete(kids_def_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_aggression_pred(np.delete(kids_def_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_aggression_pred(np.delete(kids_def_rt, tot_na,axis=0),tot_aggr)

"""Just the later 7 rounds----------------------------------------------"""
kids_tft_rt = df[['tft_rt' + str(i) for i in range(4,11)]].to_numpy()
kids_def_rt = df[['def_rt' + str(i) for i in range(4,11)]].to_numpy()
kids_coop_rt = df[['coop_rt' + str(i) for i in range(4,11)]].to_numpy()

print("\n\n\nThe average accuracy of continuous variable models when trained\n",
"using the kids' reaction time on rounds 4 to 10 is:")
print("\n1. against TFT opponent: ")
print('Proactive Agression')
continuous_variable_aggression_pred(np.delete(kids_tft_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_aggression_pred(np.delete(kids_tft_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_aggression_pred(np.delete(kids_tft_rt, tot_na,axis=0),tot_aggr)


print("\n2. against COOP opponent: ")
print('Proactive Agression')
continuous_variable_aggression_pred(np.delete(kids_coop_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_aggression_pred(np.delete(kids_coop_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_aggression_pred(np.delete(kids_coop_rt, tot_na,axis=0),tot_aggr)

print("\n3. against DEFECTIVE opponent: ")
print('Proactive Agression')
continuous_variable_aggression_pred(np.delete(kids_def_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_aggression_pred(np.delete(kids_def_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_aggression_pred(np.delete(kids_def_rt, tot_na,axis=0),tot_aggr)

"""Just 4th and 8th rounds----------------------------------------------"""
kids_def_rt = df[['def_rt' + str(i) for i in [4,8]]].to_numpy()
kids_coop_rt = df[['coop_rt' + str(i) for i in [4,8]]].to_numpy()

print("\n\n\nThe average accuracy of continuous variable models when trained\n",
"using the kids' reaction time on rounds 4 and 8 is:")

print("\n1. against TFT opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_tft_rt,proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_tft_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_tft_rt, tot_na,axis=0),tot_aggr)


print("\n2. against COOP opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_coop_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_coop_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_coop_rt, tot_na,axis=0),tot_aggr)

print("\n3. against DEFECTIVE opponent: ")
print('Proactive Agression')
continuous_variable_modeling(np.delete(kids_def_rt, proactive_na,axis=0),proactive_aggr)
print('Reactive Agression')
continuous_variable_modeling(np.delete(kids_def_rt, reactive_na,axis=0),reactive_aggr)
print('Total Agression')
continuous_variable_modeling(np.delete(kids_def_rt, tot_na,axis=0),tot_aggr)

"""The RMSE score changes a lot in each time the code is run. Also the r2 score is either negative/ greater than 1 which means the model fits the data very poorly.  """

