{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0Nz8Ydzx37l",
        "outputId": "52f6df09-4cd0-492a-fa08-cb979322d4bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05szjpWoxVny",
        "outputId": "31bb62f9-f717-4893-d825-66359caa2d2c"
      },
      "source": [
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "!pip install bcolz\n",
        "import bcolz\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt # we can also use seaborn for prettier graphs or bokeh for interactive graphs \n",
        "import statistics as stat\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 17.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.19.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2651466 sha256=df6b057ba54f5d4eb44027a20f6d0899c5f4ceecdcbb95acf86d988a49e251ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sahNmUvGx-a9"
      },
      "source": [
        "DATASET_PATH = '/content/gdrive/Shareddrives/CS506_Team/Data/Blake_RPD_Dataset-2.xlsx'\n",
        "df = pd.read_excel(DATASET_PATH, \"data\") # data is the sheet name\n",
        "tft = ['tft' + str(i) for i in range(1,11)]\n",
        "tft_rt = ['tft_rt' + str(i) for i in range(1,11)]\n",
        "coop = ['coop' + str(i) for i in range(1,11)]\n",
        "coop_rt = ['coop_rt' + str(i) for i in range(1,11)]\n",
        "defs = ['def' + str(i) for i in range(1,11)]\n",
        "def_rt = ['def_rt' + str(i) for i in range(1,11)]\n",
        "decisions = tft+coop+defs\n",
        "\n",
        "features = [tft, tft_rt, coop, coop_rt, defs, def_rt,decisions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqZ4y_Wsyak2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Converts an array of aggression scores into an array of\n",
        "0 and 1's. 0 meaning score below median, and 1 meaning score\n",
        "above median.\n",
        "We are converting the scores like this because some score groups\n",
        "are so underpopulated such that they only have 1 member \"\"\"\n",
        "def convert_by_median(aggr):\n",
        "  \"\"\"\n",
        "  :param aggr: an array of a particular aggression score\n",
        "  \"\"\"\n",
        "  med = np.median(aggr)\n",
        "  for i in range(aggr.size):\n",
        "    if aggr[i]<med:\n",
        "      aggr[i] = 0\n",
        "    else:\n",
        "      aggr[i] = 1\n",
        "  return aggr\n",
        "\n",
        "\"\"\"\n",
        "A helper function for preparing an array of aggression scores\n",
        "before training the Logictic Regression Model.\n",
        "We convert the datafram column to numpy array, finds the nan\n",
        "indices, drop the nan's, and convert the array into 0 and 1's\n",
        "by its median.\n",
        "Returns the processed array and nan indices. \"\"\"\n",
        "def get_aggr(aggr_label):\n",
        "  aggr = df[aggr_label].to_numpy(dtype=int)\n",
        "  aggr_na = np.argwhere(np.isnan(aggr))\n",
        "  aggr = aggr[~np.isnan(aggr)]\n",
        "  aggr = convert_by_median(np.nan_to_num(aggr))\n",
        "  return aggr, aggr_na\n",
        "\n",
        "\"\"\"\n",
        "Returns a matrix 20 x 3number_of_kids, with the first 10 columns being the\n",
        "partner's deicisions and the latter 10 being the kids' decisions.\"\"\"\n",
        "def prepare_data(test_percent,data):\n",
        "  \"\"\"\n",
        "  :param test_percent: percentage of test set among all samples\n",
        "  :param data: a 30 x n_kids numpy matrix of the tft, coop, and def decisions of\n",
        "  the particular kids we are interested in.\n",
        "  \"\"\"\n",
        "  tot_mat = np.empty(shape=[3*data.shape[0],20])\n",
        "  tft_dec = data[:,0:10]\n",
        "  coop_dec = data[:,10:20]\n",
        "  def_dec = data[:,20:30]\n",
        "  for i,tft in enumerate(tft_dec):\n",
        "    partner = np.copy(tft)\n",
        "    partner[1:] = partner[0:-1]\n",
        "    partner[0] = 1\n",
        "    tot_mat[i] = np.concatenate((partner,tft))\n",
        "  coop_default = np.array([1,1,0,1,1,1,0,1,1,1])\n",
        "  for i,coop in enumerate(coop_dec):\n",
        "    tot_mat[data.shape[0]+i] = np.concatenate((coop_default,coop))\n",
        "  def_default = np.array([0,0,1,0,0,0,1,0,0,0])\n",
        "  for i,defs in enumerate(def_dec):\n",
        "    tot_mat[2*data.shape[0]+i] = np.concatenate((def_default,defs))\n",
        "\n",
        "  shuffindex = np.random.permutation(3*data.shape[0])\n",
        "  tot_mat = tot_mat[shuffindex]\n",
        "  print(tot_mat[:3])\n",
        "  print(tot_mat.shape)\n",
        "  #regressiondata = np.array(data[data['period']==10].iloc[:,3:51]) # (8258, 48)\n",
        "  #regressiondata,trajs = regressiondata[shuffindex],trajs[shuffindex] \n",
        "  #train_set_rgx, train_set_rgy = ipd_regression_data(regressiondata[n:])\n",
        "  #test_set_rgx, test_set_rgy = ipd_regression_data(regressiondata[:n])\n",
        "  tot_mat = tot_mat.reshape((3*data.shape[0],2,10)) # (df.shape[0], 2, 10)\n",
        "  print(tot_mat[:3])\n",
        "  print(tot_mat.shape)\n",
        "  #trajs[trajs==0] = 2\n",
        "  #trajs = trajs - 1  # why inverting the data?\n",
        "  n = int(3*data.shape[0] * test_percent) \n",
        "  train_set,test_set = tot_mat[n:],tot_mat[:n]\n",
        "  return train_set,test_set\n",
        "\n",
        "def toLoader(train_kids, test_kids):\n",
        "  import torch\n",
        "  from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "  train_data = TensorDataset(torch.from_numpy(train_kids), torch.from_numpy(train_aggr))\n",
        "  test_data = TensorDataset(torch.from_numpy(test_kids), torch.from_numpy(test_aggr))\n",
        "\n",
        "  batch_size = 50\n",
        "\n",
        "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMpUFh_mGLpV"
      },
      "source": [
        "def do_lstm(test_percent,data):\n",
        "  \"\"\"\"\n",
        "  :param kids: the kids' decisions against a particular opponent type\n",
        "  :param aggr: array of a particular aggression score\n",
        "  \"\"\"\n",
        "  train_set,test_set = prepare_data(test_percent,data)\n",
        "    \n",
        "  #Convert to loader\n",
        "  train_loader, test_loader = toLoader(train_set, test_set)\n",
        "  #Train the LSTM model\n",
        "  model = lstmModel(2,10,2,2)\n",
        "  print(model)\n",
        "  model.to(device)\n",
        "  model = train_model(model,train_loader)\n",
        "    \n",
        "  #use the model to predict scores\n",
        "  #return the accuracy\n",
        "  return test_model(model,test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6AOqMEP4Kwm"
      },
      "source": [
        "#not used\n",
        "def test_model(model,test_loader,batch_size=50):\n",
        "  num_correct = 0\n",
        "\n",
        "  model.eval()\n",
        "  for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      output = model(inputs)\n",
        "      #pred = torch.round(output.squeeze()) #rounds the output to 0/1\n",
        "      #print(pred)\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  return test_acc\n",
        "\n",
        "#not used\n",
        "def train_model(model,train_loader):\n",
        "  model.train()\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
        "  epochs, batch_size = 10, 50\n",
        "  counter = 0\n",
        "\n",
        "  for i in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        print(model)\n",
        "        model.zero_grad()\n",
        "        print(model)\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        \"\"\"if counter%print_every == 0:\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()))\"\"\"\n",
        "  return model\n",
        "\n",
        "\n",
        "class lstmModel(nn.Module):\n",
        "    def __init__(self,in_dim,hidden_dim,out_dim,layer_num):\n",
        "        super().__init__()\n",
        "        self.lstmLayer=nn.LSTM(in_dim,hidden_dim,layer_num)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.fcLayer=nn.Linear(hidden_dim,out_dim)\n",
        "        self.weightInit = (np.sqrt(1.0/hidden_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out,_=self.lstmLayer(x)\n",
        "        out=self.relu(out)\n",
        "        out=self.fcLayer(out)\n",
        "        out = nn.Softmax(dim=-1)(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0xN8acw2wYQ"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "proactive_aggr, proactive_na = get_aggr('P-Proactive_aggr')\n",
        "reactive_aggr,reactive_na = get_aggr('P-Reactive_aggr')\n",
        "tot_aggr, tot_na = get_aggr('P-Aggression_Total')\n",
        "\n",
        "\"\"\"All 10 Rounds decisions\"\"\"\n",
        "kids_tft = df[tft].to_numpy()\n",
        "kids_def = df[defs].to_numpy()\n",
        "kids_coop = df[coop].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA4U96IRyn5o"
      },
      "source": [
        "all_dec = df[decisions].to_numpy()\n",
        "all_dec_above_med_pa = np.delete(all_dec, proactive_na,axis=0)\n",
        "to_delete = []\n",
        "for i, dec in enumerate(all_dec_above_med_pa):\n",
        "  if proactive_aggr[i]==0:\n",
        "    to_delete.append(i)\n",
        "all_dec_above_med_pa = np.delete(all_dec_above_med_pa, to_delete,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSHtrhhz2Wep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef40fddd-7ff1-4259-b400-7f48904a521e"
      },
      "source": [
        "train_set,test_set = prepare_data(0.2, all_dec_above_med_pa)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            " [1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0.]]\n",
            "(474, 20)\n",
            "[[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]]\n",
            "\n",
            " [[1. 1. 0. 1. 0. 1. 1. 0. 0. 1.]\n",
            "  [1. 0. 1. 0. 1. 1. 0. 0. 1. 0.]]]\n",
            "(474, 2, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdF1bBVuIncu",
        "outputId": "fd17f5b1-2d44-434f-aee1-9b428789a9b1"
      },
      "source": [
        "lag = 1\n",
        "n_nodes,n_layers = 10, 2\n",
        "lstm = lstmModel(2, n_nodes, 2, n_layers)\n",
        "lstm.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(lstm.parameters(), lr = 1e-2)\n",
        "n_epochs, window, batch_size = 10, 10, 100\n",
        "loss_set = []\n",
        "for ep in np.arange(n_epochs) :\n",
        "    for bc in np.arange(train_set.shape[0]/batch_size):\n",
        "        inputs = Variable(torch.from_numpy(train_set[int(bc*batch_size):int((bc+1)*batch_size)]).transpose(1,2).float())\n",
        "        target = Variable(torch.from_numpy(train_set[int(bc*batch_size):int((bc+1)*batch_size)]).transpose(1,2).float())\n",
        "        print(train_set[0].shape)\n",
        "        inputs,target = inputs.to(device),target.to(device)\n",
        "        output = lstm(inputs)\n",
        "        loss = criterion(output.squeeze()[:,:-lag,0], target[:,lag:,0])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print_loss = loss.item()\n",
        "        loss_set.append(print_loss)\n",
        "        if bc % window == 0 :\n",
        "            #print(fold)\n",
        "            print('Epoch[{}/{}], Batch[{}/{}], Loss: {:.5f}'.format(ep+1, n_epochs, bc+1, train_set.shape[0]/batch_size, print_loss))\n",
        "lstm = lstm.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 10)\n",
            "Epoch[1/10], Batch[1.0/3.8], Loss: 0.26508\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[2/10], Batch[1.0/3.8], Loss: 0.25563\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[3/10], Batch[1.0/3.8], Loss: 0.25004\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[4/10], Batch[1.0/3.8], Loss: 0.25241\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[5/10], Batch[1.0/3.8], Loss: 0.25499\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[6/10], Batch[1.0/3.8], Loss: 0.25336\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[7/10], Batch[1.0/3.8], Loss: 0.25089\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[8/10], Batch[1.0/3.8], Loss: 0.24968\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[9/10], Batch[1.0/3.8], Loss: 0.24929\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "Epoch[10/10], Batch[1.0/3.8], Loss: 0.24917\n",
            "(2, 10)\n",
            "(2, 10)\n",
            "(2, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OToBthK2L5ST"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrv9c1h5KISJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}