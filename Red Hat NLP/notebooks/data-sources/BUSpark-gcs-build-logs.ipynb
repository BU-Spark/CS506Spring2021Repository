{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efc1c5a624f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from google.colab import files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Web scraping (Kyle, due 02/26/2021)\n",
    "#     BeautifulSoup for web scraping\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "# from google.colab import files\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import dateutil\n",
    "from dateutil import parser\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import requests\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# upload file\n",
    "#uploaded = files.upload()\n",
    "#read array\n",
    "#df2 = pd.read_csv(io.BytesIO(uploaded['logs - Sheet1.csv']))\n",
    "#turn to flat list\n",
    "#array_pages = df2.to_numpy()\n",
    "#array_pages = np.ndarray.tolist(array_pages)\n",
    "#array_pages = [item for sublist in array_pages for item in sublist]\n",
    "\n",
    "#url core needed to pull\n",
    "website = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
    "base = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/\"\n",
    "ending = \"build-log.txt\"\n",
    "url = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/1300557127638585344/build-log.txt\"\n",
    "page = requests.get(base)    \n",
    "data = page.text\n",
    "soup = BeautifulSoup(data)\n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "links = links[1:-1]\n",
    "\n",
    "final_array = []\n",
    "# create array of urls\n",
    "for x in range(len(links)):\n",
    "  final_array.append(str(website) + str(links[x]) + str(ending))\n",
    "\n",
    "\n",
    "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
    "# array_of_logs[x][y] is an individual log line split by new line\n",
    "array_of_logs = []\n",
    "for x in range(len(final_array)):\n",
    "  page = urlopen(final_array[x])\n",
    "  html_bytes = page.read()\n",
    "  array_of_logs.append(str(html_bytes).split('\\\\n'))\n",
    "  \n",
    "# first log\n",
    "print(array_of_logs[0])\n",
    "\n",
    "\n",
    "# Analysis on the log data. Trying to find a framework. API (Ningxiao, Parker, Tianze, Hong)\n",
    "#     Identify limitations with data and potential risks of achieving project goals.\n",
    "\n",
    "\n",
    "# ******* Tianze *******\n",
    "#ignore useless \"Waiting for setup to finish...\"\n",
    "def ignoreWaiting(logs):\n",
    "  for i in range(len(logs)):\n",
    "    if \"Waiting for setup to finish...\" in logs[i]:\n",
    "      logs[i]=\"\"\n",
    "  return logs\n",
    "# ******* Tianze *******\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ca951e4e65cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mending2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"build-log.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msoup2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "#url core needed to pull\n",
    "website2 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
    "base2 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/\"\n",
    "ending2 = \"build-log.txt\"\n",
    "page2 = requests.get(base2)    \n",
    "data2 = page2.text\n",
    "soup2 = BeautifulSoup(data2)\n",
    "links2 = []\n",
    "for link2 in soup2.find_all('a'):\n",
    "    links2.append(link2.get('href'))\n",
    "links2 = links2[1:-1]\n",
    "\n",
    "final_array2 = []\n",
    "# create array of urls\n",
    "for x in range(len(links2)):\n",
    "  final_array2.append(str(website2) + str(links2[x]) + str(ending2))\n",
    "\n",
    "\n",
    "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
    "# array_of_logs[x][y] is an individual log line split by new line\n",
    "array_of_logs2 = []\n",
    "for x in range(len(final_array2)):\n",
    "  page2 = urlopen(final_array2[x])\n",
    "  html_bytes2 = page2.read()\n",
    "  array_of_logs2.append(str(html_bytes2).split('\\\\n'))\n",
    "  \n",
    "# first log\n",
    "print(array_of_logs2[0])\n",
    "\n",
    "\n",
    "# Analysis on the log data. Trying to find a framework. API (Ningxiao, Parker, Tianze, Hong)\n",
    "#     Identify limitations with data and potential risks of achieving project goals.\n",
    "\n",
    "\n",
    "# ******* Tianze *******\n",
    "#ignore useless \"Waiting for setup to finish...\"\n",
    "def ignoreWaiting(logs):\n",
    "  for i in range(len(logs)):\n",
    "    if \"Waiting for setup to finish...\" in logs[i]:\n",
    "      logs[i]=\"\"\n",
    "  return logs\n",
    "# ******* Tianze *******\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* PARKER *******\n",
    "#  helper function detecting if a string is a date / timestamp\n",
    "def is_date(str):\n",
    "  try:\n",
    "    dateutil.parser.parse(str)\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "# logs variable contains all parsed logs\n",
    "logs = []\n",
    "for i in range(len(array_of_logs)):\n",
    "  #  removing newline characters\n",
    "  array_of_logs[i] = array_of_logs[i].replace('\\\\n', ' ')\n",
    "  array_of_logs[i] = array_of_logs[i].splitlines()\n",
    "\n",
    "# removes leading 'b from log\n",
    "  array_of_logs[i] = array_of_logs[i][0][2:]\n",
    "# splitting each section as its own index (for parsing)\n",
    "  array_of_logs[i] = array_of_logs[i].split(' ')\n",
    "\n",
    "#  tmp is log without timestamps\n",
    "  tmp = []\n",
    "  for j in range(len(array_of_logs[i])):\n",
    "    if is_date(array_of_logs[i][j]) == False:\n",
    "      tmp.append(array_of_logs[i][j])\n",
    "    else:\n",
    "      continue\n",
    "  logs.append(tmp)\n",
    "\n",
    "# removes whitespace characters and keeps root words\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "for log in logs:\n",
    "  log[:] = [stemmer.stem(x) for x in log if x != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell removes dates and timestamps, whitespace and keeps the stems of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* PARKER *******\n",
    "failures = []\n",
    "successes = []\n",
    "# word order for matrix: 0: fail, 1: error, 2: success, 3: run, 4: crashloopbackoff\n",
    "word_matrix = []\n",
    "for log in logs:\n",
    "  tmp = ' '.join(log)\n",
    "  tmp = TextBlob(tmp)\n",
    "  fail = tmp.word_counts['fail']\n",
    "  error = tmp.word_counts['error']\n",
    "  success = tmp.word_counts['success']\n",
    "  run = tmp.word_counts['run']\n",
    "  crash = tmp.word_counts['crashloopbackoff']\n",
    "  word_matrix.append([fail, error, success, run, crash])\n",
    "\n",
    "# remove noise\n",
    "word_matrix = word_matrix[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a matrix of keywords. each row is a single log entry and each column is a specific keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* PARKER *******\n",
    "import math\n",
    "# Latent Semantic Analysis approach\n",
    "\n",
    "num_of_docs = [0 for i in range(len(word_matrix[0]))]\n",
    "for i in range(len(word_matrix)):\n",
    "  for j in range(len(word_matrix[i])):\n",
    "    if word_matrix[i][j] != 0:\n",
    "      num_of_docs[j] += 1\n",
    "for i in range(len(word_matrix)):\n",
    "  for j in range(len(word_matrix[i])):\n",
    "    tf = word_matrix[i][j]\n",
    "    idf = math.log(len(word_matrix) / num_of_docs[j])\n",
    "    word_matrix[i][j] = tf * idf\n",
    "\n",
    "# TODO: try to implement anomaly detection with word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* NINGXIAO *******\n",
    "# find events in log, display frequecy of each event in a bar chart\n",
    "import spacy\n",
    "s = \"\"\n",
    "for log in logs:\n",
    "    temp = \"\"\n",
    "    temp = temp.join(log)\n",
    "    s += temp\n",
    "# Error Message: Text of length 5496377 exceeds maximum of 1000000.\n",
    "s = s[0:100000]\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(s)\n",
    "dictionary = {};\n",
    "for token in doc:\n",
    "    if token.pos_ == 'VERB':\n",
    "        if token.text not in dictionary:\n",
    "            dictionary[token.text] = 1\n",
    "        else:\n",
    "            dictionary[token.text] += 1\n",
    "    #print(token.text, toekn.pos_)\n",
    "\n",
    "#print(dictionary)\n",
    "\n",
    "keys = list(dictionary.keys())\n",
    "values = list(dictionary.values())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9.2, 7))  \n",
    "ax.barh(keys,values)\n",
    "plt.title('Verbs in logs')\n",
    "plt.ylabel('Verbs')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above shows the verbs and their frequency appear in the tokenized log text. It indicates what event and how often it happens in the log."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
