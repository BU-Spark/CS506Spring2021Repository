{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import os\n",
    "import re\n",
    "from gensim import corpora, models\n",
    "import jieba.posseg as jp\n",
    "import jieba\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data/'\n",
    "All_notes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    xlsx = xlrd.open_workbook(path+filename)\n",
    "    sheet1 = xlsx.sheets()[0]\n",
    "    for i in range(6,sheet1.nrows-1):\n",
    "        All_notes.append(sheet1.row_values(i)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_notes=[nlp(note) for note in All_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_notes=[[i for i in note.sents] for note in spa_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_notes[1][0][1].pos_=='NOUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 'NN'), ('o', 'NN')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_notes=[]\n",
    "for note in sents_notes:\n",
    "    nt = []\n",
    "    for sent in note:\n",
    "        st = []\n",
    "        for token in sent:\n",
    "            if nlp.vocab[token.lemma_].is_stop==False:\n",
    "                st.append(token.lemma_.lower())\n",
    "        nt.append(st.copy())\n",
    "    token_notes.append(nt.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tokens=[]\n",
    "for i in token_notes:\n",
    "    split_tokens+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = u'[0-9’!\"#$%&\\' \\n()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pured_split_tokens = []\n",
    "for sent in split_tokens:\n",
    "    tar=[word for word in sent if re.sub(r1, '', word)==word and len(word)>2]\n",
    "    if tar:\n",
    "        pured_split_tokens.append(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成LDA模型\n",
    "def LDA_model(words_list):\n",
    "    # 构造词典\n",
    "    # Dictionary()方法遍历所有的文本，为每个不重复的单词分配一个单独的整数ID，同时收集该单词出现次数以及相关的统计信息\n",
    "    dictionary = corpora.Dictionary(words_list)\n",
    "#     print(dictionary)\n",
    "#     print('打印查看每个单词的id:')\n",
    "#     print(dictionary.token2id)  # 打印查看每个单词的id\n",
    " \n",
    "    # 将dictionary转化为一个词袋\n",
    "    # doc2bow()方法将dictionary转化为一个词袋。得到的结果corpus是一个向量的列表，向量的个数就是文档数。\n",
    "    # 在每个文档向量中都包含一系列元组,元组的形式是（单词 ID，词频）\n",
    "    corpus = [dictionary.doc2bow(words) for words in words_list]\n",
    "#     print('输出每个文档的向量:')\n",
    "#     print(corpus)  # 输出每个文档的向量\n",
    " \n",
    "    # LDA主题模型\n",
    "    # num_topics -- 必须，要生成的主题个数。\n",
    "    # id2word    -- 必须，LdaModel类要求我们之前的dictionary把id都映射成为字符串。\n",
    "    # passes     -- 可选，模型遍历语料库的次数。遍历的次数越多，模型越精确。但是对于非常大的语料库，遍历太多次会花费很长的时间。\n",
    "    lda_model = models.ldamodel.LdaModel(corpus=corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    " \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印所有主题，每个主题显示5个词:\n",
      "[(0, '0.048*\"mom\" + 0.035*\"school\" + 0.034*\"case\" + 0.022*\"meeting\" + 0.021*\"week\"'), (1, '0.032*\"time\" + 0.020*\"family\" + 0.017*\"court\" + 0.014*\"home\" + 0.013*\"thing\"')]\n",
      "输出该主题的的词及其词的权重:\n",
      "[('mom', 0.048121765), ('school', 0.03487527), ('case', 0.034221936), ('meeting', 0.022419142), ('week', 0.021112563), ('phone', 0.02036473), ('date', 0.017823054), ('message', 0.017481834), ('email', 0.016758647), ('issue', 0.013176496), ('report', 0.012495851), ('day', 0.012445006), ('police', 0.011745162), ('point', 0.008995797), ('note', 0.008931411), ('cell', 0.008251044), ('work', 0.007342673), ('office', 0.0072296956), ('staff', 0.0068044835), ('session', 0.0064216596)]\n"
     ]
    }
   ],
   "source": [
    "# 获取训练后的LDA模型\n",
    "lda_model = LDA_model(pured_split_tokens)\n",
    "\n",
    "# 可以用 print_topic 和 print_topics 方法来查看主题\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "topic_words = lda_model.print_topics(num_topics=5, num_words=5)\n",
    "print('打印所有主题，每个主题显示5个词:')\n",
    "print(topic_words)\n",
    "\n",
    "# 输出该主题的的词及其词的权重\n",
    "words_list = lda_model.show_topic(0, 20)\n",
    "print('输出该主题的的词及其词的权重:')\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, KneserNeyInterpolated\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_train_data, trigram_padded_sents = padded_everygram_pipeline(3, pured_split_tokens)\n",
    "trigram = MLE(3)\n",
    "trigram.fit(trigram_train_data, trigram_padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/alvations/n-gram-language-model-with-nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=7):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reveal tell desmond desmond want ask visit bail raise\n",
      "\n",
      "unit wednesday ice cube cafeteria worker woman\n",
      "attach correspondence expert surveillance datum\n",
      "house\n",
      "goal set meeting mom ready school\n",
      "phone revocation attorney\n",
      "present\n",
      "typically meet danequa family\n",
      "bjc clerk office require fund request\n"
     ]
    }
   ],
   "source": [
    "#generate sentence\n",
    "for i in range(10):\n",
    "    print(generate_sent(trigram, 20, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 50 words in POS,  ['desmond', 'talk', 'school', 'mom', 'want', 'tell', 'know', 'need', 'case', 'work', 'time', 'court', 'come', 'like', 'home', 'day', 'report', 'think', 'week', 'dys', 'leave', 'try', 'email', 'note', 'good', 'phone', 'let', 'date', 'police', 'check', 'issue', 'hear', 'send', 'ziyad', 'look', 'house', 'speak', 'family', 'help', 'meet', 'feel', 'today', 'meeting', 'thing', 'kid', 'start', 'tomorrow', 'explain', 'sure', 'new']\n"
     ]
    }
   ],
   "source": [
    "D=dict()\n",
    "for sent in pured_split_tokens:\n",
    "    for word in sent:\n",
    "        if (ord(\"a\")<ord(word[0])<=ord(\"z\") or ord(\"A\")<ord(word[0])<=ord(\"Z\")):\n",
    "            if word not in D:\n",
    "                D[word]=1\n",
    "            else:\n",
    "                D[word]+=1\n",
    "DL=sorted(list(D.items()),key=lambda x:-x[1])\n",
    "print(\"Most 50 words in POS, \",[DL[i][0] for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 50 nouns in POS,  ['desmond', 'talk', 'school', 'mom', 'want', 'tell', 'need', 'case', 'work', 'time', 'court', 'home', 'day', 'report', 'think', 'week', 'dys', 'try', 'email', 'note', 'phone', 'date', 'check', 'issue', 'hear', 'send', 'ziyad', 'look', 'house', 'speak', 'family', 'help', 'meet', 'feel', 'today', 'meeting', 'thing', 'kid', 'start', 'tomorrow', 'explain', 'sure', 'probation', 'thank', 'charge', 'officer', 'visit', 'message', 'hour', 'community', 'mother', 'use', 'father', 'schedule', 'jaden', 'record', 'bail', 'motion', 'maureen', 'danielle', 'month', 'point', 'peter', 'confirm', 'stay', 'update', 'hope', 'bring', 'pass', 'file', 'request', 'end', 'plan', 'window', 'release', 'office', 'cop', 'reach', 'year', 'judge', 'gps', 'boston', 'class', 'hearing', 'friend', 'clinician', 'write', 'remind']\n"
     ]
    }
   ],
   "source": [
    "print(\"Most 50 nouns in POS, \",[DL[i][0] for i in range(100) if nltk.pos_tag([DL[i][0]])[0][1]==\"NN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tell', 'NN')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"fee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 50 nouns in POS,  ['know', 'come', 'leave', 'let', 'find', 'happen']\n"
     ]
    }
   ],
   "source": [
    "print(\"Most 50 verbs in POS, \",[DL[i][0] for i in range(100) if nltk.pos_tag([DL[i][0]])[0][1]==\"VB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 50 nouns in POS,  ['good', 'new', 'open']\n"
     ]
    }
   ],
   "source": [
    "print(\"Most 50 JJ in POS, \",[DL[i][0] for i in range(100) if nltk.pos_tag([DL[i][0]])[0][1]==\"JJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 50 nouns in POS,  ['like']\n"
     ]
    }
   ],
   "source": [
    "print(\"Most 50 IN in POS, \",[DL[i][0] for i in range(100) if nltk.pos_tag([DL[i][0]])[0][1]==\"IN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
