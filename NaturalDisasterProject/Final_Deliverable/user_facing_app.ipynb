{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename1 = 'model_two_weeks.sav'\n",
    "filename2 = 'model_one_week.sav'\n",
    "filename3 = 'model_three_days.sav'\n",
    "filename4 = 'model_one_day.sav'\n",
    "model1 = pickle.load(open(filename1, 'rb'))\n",
    "model2 = pickle.load(open(filename2, 'rb'))\n",
    "model3 = pickle.load(open(filename3 'rb'))\n",
    "model4 = pickle.load(open(filename4, 'rb'))\n",
    "\n",
    "# result = loaded_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load scalars from disk\n",
    "filename1 = 'scalar_two_weeks.sav'\n",
    "filename2 = 'scalar_one_week.sav'\n",
    "filename3 = 'scalar_three_days.sav'\n",
    "filename4 = 'scalar_one_day.sav'\n",
    "scalar1 = pickle.load(open(filename1, 'rb'))\n",
    "scalar2 = pickle.load(open(filename2, 'rb'))\n",
    "scalar3 = pickle.load(open(filename3 'rb'))\n",
    "scalar4 = pickle.load(open(filename4, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine slope of each weather attribute over time for each Tornado/Event\n",
    "def slopes (start, end, data):\n",
    "    temp_change = []\n",
    "    wind_change = []\n",
    "    rain_change = []\n",
    "    radiation_change = []\n",
    "    pressure_change = []\n",
    "    humidity_change = []\n",
    "    x = list(range(start-end))\n",
    "\n",
    "    for i in event_ids:\n",
    "        event = df.loc[df['event_id'] == i]\n",
    "        temp = event['temperature']\n",
    "        wind = event['wind_speed']\n",
    "        radiation = event['surface_solar_radiation']\n",
    "        humidity = event['relative_humidity']\n",
    "        pressure = event['surface_pressure']\n",
    "        rain = event['total_precipitation']\n",
    "\n",
    "        t_slope, intercept, r_value, p_value, std_err = stats.linregress(x, temp[-start:-end])\n",
    "        w_slope, intercept, r_value, p_value, std_err = stats.linregress(x, wind[-start:-end])\n",
    "        r_slope, intercept, r_value, p_value, std_err = stats.linregress(x, rain[-start:-end])\n",
    "        rd_slope, intercept, r_value, p_value, std_err = stats.linregress(x, radiation[-start:-end])\n",
    "        p_slope, intercept, r_value, p_value, std_err = stats.linregress(x, pressure[-start:-end])\n",
    "        h_slope, intercept, r_value, p_value, std_err = stats.linregress(x, humidity[-start:-end])\n",
    "\n",
    "        temp_change.append(t_slope)\n",
    "        wind_change.append(w_slope)\n",
    "        rain_change.append(r_slope)\n",
    "        radiation_change.append(rd_slope)\n",
    "        pressure_change.append(p_slope)\n",
    "        humidity_change.append(h_slope)\n",
    "    return temp_change, wind_change, rain_change, radiation_change, pressure_change, humidity_change\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(start, end, data, wk_two_pred=None, wk_one_pred=None, day_3_pred=None):\n",
    "\n",
    "    t1 = data.groupby('event_id').tail(start)\n",
    "    t2 = t1.groupby('event_id').head(end)\n",
    "\n",
    "    averages = t2.groupby('event_id').mean()\n",
    "    averages['t_slope'] = temp_change\n",
    "    averages['w_slope'] = wind_change\n",
    "    averages['r_slope'] = rain_change\n",
    "    averages['rd_slope'] = radiation_change\n",
    "    averages['p_slope'] = pressure_change\n",
    "    averages['h_slope'] = humidity_change\n",
    "    X = averages[['temperature','wind_speed','surface_solar_radiation', 'total_precipitation', 't_slope', 'w_slope', 'r_slope', 'rd_slope', 'h_slope']]\n",
    "    y = averages['outcome']\n",
    "\n",
    "    if wk_two_pred is not None:\n",
    "        averages['two_week_pred'] = two_pred_all\n",
    "        X = averages[['temperature','wind_speed','surface_solar_radiation', 'total_precipitation', 't_slope', 'w_slope', 'r_slope', 'rd_slope', 'h_slope', 'two_week_pred']]\n",
    "\n",
    "    if wk_one_pred is not None:  \n",
    "        averages['one_week_pred'] = one_pred_all\n",
    "        X = averages[['temperature','wind_speed','surface_solar_radiation', 'total_precipitation', 't_slope', 'w_slope', 'r_slope', 'rd_slope', 'h_slope', 'two_week_pred', 'one_week_pred']]\n",
    "        \n",
    "    if day_3_pred is not None:\n",
    "        averages['3_day_pred'] = day3_pred_all2\n",
    "        X = averages[['temperature','wind_speed','surface_solar_radiation', 'total_precipitation', 't_slope', 'w_slope', 'r_slope', 'rd_slope', 'h_slope', 'two_week_pred', 'one_week_pred', 'day_3_pred']]\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement user input method"
   ]
  }
 ]
}