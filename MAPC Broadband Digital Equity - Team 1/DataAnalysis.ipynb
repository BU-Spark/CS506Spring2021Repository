{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code block for imports \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('cs506MAPC.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display first 10 Providers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrystr =\"SELECT DISTINCT t1.ProviderName,substr(t1.BlockCode,0,12) as tractNum,t1.MaxAdDown,t1.MaxAdUp, t2.town, t3.Longitude, t3.Latitude FROM FCC_DATA_T t1, censusblocks_t t2, CityLongLat_t t3 WHERE substr(t1.BlockCode,0,12) = t2.blockcode AND t2.town = t3.town ORDER BY t2.town ASC\"\n",
    "\n",
    "\n",
    "#convert query results into dataframe\n",
    "df_filtered_providers = pd.read_sql_query(qrystr, conn)\n",
    "print(df_filtered_providers.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Zero Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryStr2 = \"SELECT DISTINCT t1.ProviderName,substr(t1.BlockCode,0,12) as tractNum,t1.MaxAdDown,t1.MaxAdUp, t2.town, t3.Longitude, t3.Latitude FROM FCC_DATA_T t1, censusblocks_t t2, CityLongLat_t t3 WHERE MaxAdDown != 0.0 AND MaxAdUp != 0.0 and substr(t1.BlockCode,0,12) = t2.blockcode AND t2.town = t3.town ORDER BY t2.town ASC\"\n",
    "df_filtered_non_zero_providers = pd.read_sql_query(qryStr2, conn)\n",
    "print(df_filtered_non_zero_providers.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryStr3 = \"SELECT DISTINCT t1.ProviderName,substr(t1.BlockCode,0,12) as tractNum,t1.MaxAdDown,t1.MaxAdUp, t2.town, t3.Longitude, t3.Latitude FROM FCC_DATA_T t1, censusblocks_t t2, CityLongLat_t t3 WHERE MaxAdDown = 0 AND MaxAdUp = 0 and substr(t1.BlockCode,0,12) = t2.blockcode AND t2.town = t3.town ORDER BY t2.town ASC\"\n",
    "df_filtered_zero_providers = pd.read_sql_query(qryStr3, conn)\n",
    "print(df_filtered_zero_providers.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Providers Per Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "town_dict = {}\n",
    "towns = df_filtered_zero_providers['town']\n",
    "providers = df_filtered_zero_providers['ProviderName']\n",
    "df_towns_providers = pd.concat([towns, providers], axis=1)\n",
    "numpy_arr = df_towns_providers.to_numpy()\n",
    "for i in range(len(numpy_arr)):\n",
    "    town_name = numpy_arr[i][0]\n",
    "    if town_name not in town_dict.keys():\n",
    "        town_dict[town_name] = [numpy_arr[i][1]]\n",
    "    elif numpy_arr[i][1] not in town_dict.get(town_name):\n",
    "        town_dict[town_name].append(numpy_arr[i][1])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "num_providers_dict = {}\n",
    "for key in town_dict.keys():\n",
    "    num_providers_dict[key] = len(town_dict.get(key))\n",
    "\n",
    "df_num_providers = pd.DataFrame(num_providers_dict.items(), columns=['Town', 'ProviderCount'])\n",
    "print(df_num_providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display top/bottom 5 areas by number of providers (& Revere, Everett, Quincy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_most_providers = dict(sorted(num_providers_dict.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "five_least_providers = dict(sorted(num_providers_dict.items(), key=lambda item: item[1])[:5])\n",
    "everett_providers = {'EVERETT': num_providers_dict.get('EVERETT')}\n",
    "quincy_providers = {'QUINCY': num_providers_dict.get('QUINCY')}\n",
    "revere_providers = {'REVERE': num_providers_dict.get('REVERE')}\n",
    "plot_providers = {**five_most_providers, **five_least_providers, **everett_providers, **quincy_providers, **revere_providers}\n",
    "df_plot_providers = pd.DataFrame(plot_providers.items(), columns=['Town', 'ProviderCount'])\n",
    "\n",
    "y_max_tick = max(plot_providers.values())\n",
    "\n",
    "print(df_plot_providers)\n",
    "df_plot_providers.plot(x='Town', y='ProviderCount', kind='bar')\n",
    "plt.yticks(np.arange(0, y_max_tick, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxAdDown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryStr4 = \"SELECT DISTINCT t1.MaxAdDown, t2.town as Town FROM FCC_DATA_T t1, censusblocks_t t2, CityLongLat_t t3 WHERE MaxAdDown != 0.0 AND MaxAdUp != 0.0 and substr(t1.BlockCode,0,12) = t2.blockcode AND t2.town = t3.town GROUP BY t2.town ORDER BY MaxAdDown DESC\"\n",
    "df_filtered_max_down = pd.read_sql_query(qryStr4, conn)\n",
    "print(df_filtered_max_down.head(10))\n",
    "\n",
    "top_ten = df_filtered_max_down.nlargest(10, ['MaxAdDown'])\n",
    "bottom_ten = df_filtered_max_down.nsmallest(10, ['MaxAdDown'])\n",
    "revere = df_filtered_max_down.loc[df_filtered_max_down['Town'] == 'REVERE']\n",
    "quincy = df_filtered_max_down.loc[df_filtered_max_down['Town'] == 'QUINCY']\n",
    "everett = df_filtered_max_down.loc[df_filtered_max_down['Town'] == 'EVERETT']\n",
    "\n",
    "combined_max_down_top = pd.concat([top_ten, revere, quincy, everett], axis=0)\n",
    "combined_max_down_top = combined_max_down_top.reset_index(drop=True)\n",
    "combined_max_down_bottom = pd.concat([bottom_ten, revere, quincy, everett], axis=0)\n",
    "combined_max_down_bottom = combined_max_down_bottom.reset_index(drop=True)\n",
    "\n",
    "combined_max_down_top.plot(x='Town', y='MaxAdDown', kind='bar', title = \"Top 10 Town Download Speeds\")\n",
    "plt.show()\n",
    "\n",
    "combined_max_down_bottom.plot(x='Town', y='MaxAdDown', kind='bar', title = \"Bottom 10 Town Download Speeds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxAdUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryStr5 = \"SELECT DISTINCT t1.MaxAdUp, t2.town as Town FROM FCC_DATA_T t1, censusblocks_t t2, CityLongLat_t t3 WHERE MaxAdDown != 0.0 AND MaxAdUp != 0.0 and substr(t1.BlockCode,0,12) = t2.blockcode AND t2.town = t3.town GROUP BY t2.town ORDER BY MaxAdUp DESC\"\n",
    "df_filtered_max_up = pd.read_sql_query(qryStr5, conn)\n",
    "print(df_filtered_max_up.head(10))\n",
    "\n",
    "top_ten = df_filtered_max_up.nlargest(10, ['MaxAdUp'])\n",
    "bottom_ten = df_filtered_max_up.nsmallest(10, ['MaxAdUp'])\n",
    "revere = df_filtered_max_up.loc[df_filtered_max_up['Town'] == 'REVERE']\n",
    "quincy = df_filtered_max_up.loc[df_filtered_max_up['Town'] == 'QUINCY']\n",
    "everett = df_filtered_max_up.loc[df_filtered_max_up['Town'] == 'EVERETT']\n",
    "\n",
    "combined_max_up_top = pd.concat([top_ten, revere, quincy, everett], axis=0)\n",
    "combined_max_up_top = combined_max_up_top.reset_index(drop=True)\n",
    "combined_max_up_bottom = pd.concat([bottom_ten, revere, quincy, everett], axis=0)\n",
    "combined_max_up_bottom = combined_max_up_bottom.reset_index(drop=True)\n",
    "\n",
    "combined_max_up_top.plot(x='Town', y='MaxAdUp', kind='bar', title = \"Top 10 Town Upload Speeds\")\n",
    "plt.show()\n",
    "\n",
    "combined_max_up_bottom.plot(x='Town', y='MaxAdUp', kind='bar', title = \"Bottom 10 Town Upload Speeds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Income  (Census Tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryStr6 = \"SELECT t2.Town as Town, AVG(t1.mhi) as 'Average Median Income' FROM median_income_t t1, censusblocks_t t2 WHERE t2.blockcode = t1.ct10_id and t1.mhi > 1 GROUP BY t2.Town;\"\n",
    "town_median_income  = pd.read_sql_query(qryStr6, conn)\n",
    "town_median_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_median_income.sort_values(by=['Average Median Income'])\n",
    "top_five_towns = town_median_income.nlargest(5, ['Average Median Income'])\n",
    "bottom_five_towns = town_median_income.nsmallest(5, ['Average Median Income'])\n",
    "revere = town_median_income.loc[town_median_income['Town'] == 'REVERE']\n",
    "quincy = town_median_income.loc[town_median_income['Town'] == 'QUINCY']\n",
    "everett = town_median_income.loc[town_median_income['Town'] == 'EVERETT']\n",
    "combined_median_income_df = pd.concat([top_five_towns, bottom_five_towns, revere, quincy, everett], axis=0)\n",
    "combined_median_income_df = combined_median_income_df.reset_index(drop=True)\n",
    "combined_median_income_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_median_income_df.plot(x='Town', y='Average Median Income', kind='bar') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryStr7 = \"SELECT t2.Town as Town, ROUND(AVG(t1.MaxAdDown),2) as AverageMaxDown, ROUND(AVG(t1.MaxAdUp),2) as AverageMaxUp FROM fcc_data_t t1, censusblocks_t t2 WHERE substr(t1.BlockCode,0,12) = t2.blockcode GROUP BY t2.TOWN\"\n",
    "town_average_speeds  = pd.read_sql_query(qryStr7, conn)\n",
    "town_average_speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dover = town_average_speeds.loc[town_average_speeds['Town'] == 'DOVER']\n",
    "wellesley = town_average_speeds.loc[town_average_speeds['Town'] == 'WELLESLEY']\n",
    "boxford = town_average_speeds.loc[town_average_speeds['Town'] == 'BOXFORD']\n",
    "lexington = town_average_speeds.loc[town_average_speeds['Town'] == 'LEXINGTON']\n",
    "hopkinton = town_average_speeds.loc[town_average_speeds['Town'] == 'HOPKINTON']\n",
    "springfield = town_average_speeds.loc[town_average_speeds['Town'] == 'SPRINGFIELD']\n",
    "north_adams = town_average_speeds.loc[town_average_speeds['Town'] == 'NORTH ADAMS']\n",
    "lawrence = town_average_speeds.loc[town_average_speeds['Town'] == 'LAWRENCE']\n",
    "holyoke = town_average_speeds.loc[town_average_speeds['Town'] == 'HOLYOKE']\n",
    "fall_river = town_average_speeds.loc[town_average_speeds['Town'] == 'FALL RIVER']\n",
    "revere = town_average_speeds.loc[town_average_speeds['Town'] == 'REVERE']\n",
    "quincy = town_average_speeds.loc[town_average_speeds['Town'] == 'QUINCY']\n",
    "everett = town_average_speeds.loc[town_average_speeds['Town'] == 'EVERETT']\n",
    "\n",
    "combined_average_speed_df = pd.concat([dover, wellesley, boxford, lexington, hopkinton, springfield, north_adams, lawrence, holyoke, fall_river, revere, quincy, everett], axis=0)\n",
    "combined_average_speed_df = combined_average_speed_df.reset_index(drop=True)\n",
    "combined_average_speed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_average_speed_df.plot(x='Town', y='AverageMaxDown', kind='bar') \n",
    "plt.show()\n",
    "\n",
    "# add Average Max Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_average_speed_df.plot(x='Town', y='AverageMaxUp', kind='bar') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income_and_speed_df = combined_average_speed_df\n",
    "median_income_and_speed_df['Average Median Income'] = combined_median_income_df['Average Median Income']\n",
    "median_income_and_speed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = median_income_and_speed_df.plot(kind='scatter', x='Average Median Income', y='AverageMaxDown', title='Median income vs Average Max Down Speed',)    \n",
    "\n",
    "# annotate points in axis\n",
    "for idx, row in median_income_and_speed_df.iterrows():\n",
    "    ax1.annotate(row['Town'], (row['Average Median Income'], row['AverageMaxDown']))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = median_income_and_speed_df[:6].plot(kind='scatter', x='Average Median Income', y='AverageMaxUp', title='Median income vs Average Max Up Speed',)    \n",
    "\n",
    "# annotate points in axis\n",
    "for idx, row in median_income_and_speed_df[:6].iterrows():\n",
    "    ax2.annotate(row['Town'], (row['Average Median Income'], row['AverageMaxUp']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = median_income_and_speed_df[7:].plot(kind='scatter', x='Average Median Income', y='AverageMaxUp', title='Median income vs Average Max Up Speed',)    \n",
    "\n",
    "# annotate points in axis\n",
    "for idx, row in median_income_and_speed_df[7:].iterrows():\n",
    "    ax3.annotate(row['Town'], (row['Average Median Income'], row['AverageMaxUp']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thousand-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Town  AverageMaxDown  AverageMaxUp   nh_p  nhwhi_p  nhaa_p  \\\n",
      "0         ABINGTON          316.49        141.37  97.81    88.89    1.52   \n",
      "1  ACUSHNET CENTER          207.52         30.63  95.95    88.68    0.11   \n",
      "2            ADAMS          216.09          9.17  98.89    96.31    0.68   \n",
      "3      AGAWAM TOWN          188.94          7.52  94.40    89.28    1.78   \n",
      "4    AMESBURY TOWN          178.74          7.03  97.32    92.45    0.63   \n",
      "5   AMHERST CENTER          172.43          7.37  93.46    71.55    5.89   \n",
      "6          ANDOVER          273.03        119.54  96.63    80.41    3.22   \n",
      "7        ARLINGTON          394.22        123.11  95.01    76.77    2.69   \n",
      "8            ATHOL          247.04          9.72  95.07    91.44    0.49   \n",
      "9        ATTLEBORO          194.69          8.92  92.25    82.03    3.25   \n",
      "\n",
      "   nhna_p  nhas_p  nhpi_p  nhoth_p  nhmlt_p  lat_p  \n",
      "0    0.00    3.89    0.00     0.98     2.54   2.19  \n",
      "1    0.00    1.43    0.00     2.65     3.09   4.05  \n",
      "2    0.04    0.63    0.00     0.12     1.10   1.11  \n",
      "3    0.02    2.04    0.00     0.00     1.27   5.60  \n",
      "4    0.09    0.89    0.00     0.15     3.11   2.68  \n",
      "5    0.25   11.33    0.12     0.13     4.20   6.54  \n",
      "6    0.03   10.76    0.00     0.05     2.14   3.37  \n",
      "7    0.03   11.54    0.00     0.24     3.74   4.99  \n",
      "8    0.07    0.92    0.00     0.00     2.15   4.93  \n",
      "9    0.03    4.80    0.00     0.07     2.08   7.75  \n"
     ]
    }
   ],
   "source": [
    "qryStr6 =\"SELECT t2.Town as Town, ROUND(AVG(t1.MaxAdDown),2) as AverageMaxDown, ROUND(AVG(t1.MaxAdUp),2) as AverageMaxUp,ROUND(AVG(t3.nh_p),2) as nh_p, ROUND(AVG(t3.nhwhi_p),2) as nhwhi_p,ROUND(AVG(t3.nhaa_p),2) as nhaa_p, ROUND(AVG(t3.nhna_p),2) as nhna_p,ROUND(AVG(t3.nhas_p),2) as nhas_p, ROUND(AVG(t3.nhpi_p),2) as nhpi_p,ROUND(AVG(t3.nhoth_p),2) as nhoth_p, ROUND(AVG(t3.nhmlt_p),2) as nhmlt_p,ROUND(AVG(t3.lat_p),2) as lat_p FROM fcc_data_t t1, censusblocks_t t2, race_ethnicity_t t3 WHERE substr(t1.BlockCode,0,12) = t2.blockcode  AND (t1.MaxAdDown > 0 AND t1.MaxAdUp > 0) AND t2.blockcode = t3.ct10_id GROUP BY t2.TOWN\"\n",
    "df_averages_percentage = pd.read_sql_query(qryStr6, conn)\n",
    "print(df_averages_percentage.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tested-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_averages_percentage = df_averages_percentage.rename(columns={\"nh_p\":\"Non-Hispanic\",\"nhwhi_p\": \"Non-Hispanic White\",\n",
    "                                      \"nhaa_p\":\"Non-Hispanic Black or African American\",\"nhna_p\": \"Non-Hispanic American Indian and Alaska Native\",\n",
    "                                      \"nhas_p\":\"Non-Hispanic Asian\",\"nhpi_p\": \"Non-Hispanic Native Hawaiian/Other Pacific Islander\",\n",
    "                                      \"nhoth_p\":\"Non-Hispanic Some Other Races\",\"nhmlt_p\": \"Non-Hispanic Two or More Races\",\n",
    "                                      \"lat_p\":\"Hispanic/Latino\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "advised-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_averages_percentage.to_csv('MaxSpeed_Race.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "compatible-orleans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Town</th>\n",
       "      <th>AverageMaxDown</th>\n",
       "      <th>AverageMaxUp</th>\n",
       "      <th>Non-Hispanic</th>\n",
       "      <th>Non-Hispanic White</th>\n",
       "      <th>Non-Hispanic Black or African American</th>\n",
       "      <th>Non-Hispanic American Indian and Alaska Native</th>\n",
       "      <th>Non-Hispanic Asian</th>\n",
       "      <th>Non-Hispanic Native Hawaiian/Other Pacific Islander</th>\n",
       "      <th>Non-Hispanic Some Other Races</th>\n",
       "      <th>Non-Hispanic Two or More Races</th>\n",
       "      <th>Hispanic/Latino</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABINGTON</td>\n",
       "      <td>316.49</td>\n",
       "      <td>141.37</td>\n",
       "      <td>97.81</td>\n",
       "      <td>88.89</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACUSHNET CENTER</td>\n",
       "      <td>207.52</td>\n",
       "      <td>30.63</td>\n",
       "      <td>95.95</td>\n",
       "      <td>88.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.09</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADAMS</td>\n",
       "      <td>216.09</td>\n",
       "      <td>9.17</td>\n",
       "      <td>98.89</td>\n",
       "      <td>96.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGAWAM TOWN</td>\n",
       "      <td>188.94</td>\n",
       "      <td>7.52</td>\n",
       "      <td>94.40</td>\n",
       "      <td>89.28</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>5.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMESBURY TOWN</td>\n",
       "      <td>178.74</td>\n",
       "      <td>7.03</td>\n",
       "      <td>97.32</td>\n",
       "      <td>92.45</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>WINCHENDON</td>\n",
       "      <td>143.58</td>\n",
       "      <td>5.82</td>\n",
       "      <td>97.02</td>\n",
       "      <td>93.09</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>WINCHESTER</td>\n",
       "      <td>328.73</td>\n",
       "      <td>146.88</td>\n",
       "      <td>98.00</td>\n",
       "      <td>82.48</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>WINTHROP TOWN</td>\n",
       "      <td>212.94</td>\n",
       "      <td>8.20</td>\n",
       "      <td>91.27</td>\n",
       "      <td>87.89</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.37</td>\n",
       "      <td>8.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>WOBURN</td>\n",
       "      <td>357.69</td>\n",
       "      <td>117.77</td>\n",
       "      <td>94.81</td>\n",
       "      <td>76.50</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.75</td>\n",
       "      <td>5.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>WORCESTER</td>\n",
       "      <td>228.06</td>\n",
       "      <td>9.11</td>\n",
       "      <td>80.93</td>\n",
       "      <td>58.02</td>\n",
       "      <td>12.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.16</td>\n",
       "      <td>19.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Town  AverageMaxDown  AverageMaxUp  Non-Hispanic  \\\n",
       "0           ABINGTON          316.49        141.37         97.81   \n",
       "1    ACUSHNET CENTER          207.52         30.63         95.95   \n",
       "2              ADAMS          216.09          9.17         98.89   \n",
       "3        AGAWAM TOWN          188.94          7.52         94.40   \n",
       "4      AMESBURY TOWN          178.74          7.03         97.32   \n",
       "..               ...             ...           ...           ...   \n",
       "183       WINCHENDON          143.58          5.82         97.02   \n",
       "184       WINCHESTER          328.73        146.88         98.00   \n",
       "185    WINTHROP TOWN          212.94          8.20         91.27   \n",
       "186           WOBURN          357.69        117.77         94.81   \n",
       "187        WORCESTER          228.06          9.11         80.93   \n",
       "\n",
       "     Non-Hispanic White  Non-Hispanic Black or African American  \\\n",
       "0                 88.89                                    1.52   \n",
       "1                 88.68                                    0.11   \n",
       "2                 96.31                                    0.68   \n",
       "3                 89.28                                    1.78   \n",
       "4                 92.45                                    0.63   \n",
       "..                  ...                                     ...   \n",
       "183               93.09                                    0.83   \n",
       "184               82.48                                    0.29   \n",
       "185               87.89                                    1.20   \n",
       "186               76.50                                    6.70   \n",
       "187               58.02                                   12.09   \n",
       "\n",
       "     Non-Hispanic American Indian and Alaska Native  Non-Hispanic Asian  \\\n",
       "0                                              0.00                3.89   \n",
       "1                                              0.00                1.43   \n",
       "2                                              0.04                0.63   \n",
       "3                                              0.02                2.04   \n",
       "4                                              0.09                0.89   \n",
       "..                                              ...                 ...   \n",
       "183                                            0.00                1.00   \n",
       "184                                            0.00               12.12   \n",
       "185                                            0.00                0.61   \n",
       "186                                            0.00                8.17   \n",
       "187                                            0.20                8.04   \n",
       "\n",
       "     Non-Hispanic Native Hawaiian/Other Pacific Islander  \\\n",
       "0                                                 0.00     \n",
       "1                                                 0.00     \n",
       "2                                                 0.00     \n",
       "3                                                 0.00     \n",
       "4                                                 0.00     \n",
       "..                                                 ...     \n",
       "183                                               0.00     \n",
       "184                                               0.00     \n",
       "185                                               0.00     \n",
       "186                                               0.00     \n",
       "187                                               0.03     \n",
       "\n",
       "     Non-Hispanic Some Other Races  Non-Hispanic Two or More Races  \\\n",
       "0                             0.98                            2.54   \n",
       "1                             2.65                            3.09   \n",
       "2                             0.12                            1.10   \n",
       "3                             0.00                            1.27   \n",
       "4                             0.15                            3.11   \n",
       "..                             ...                             ...   \n",
       "183                           0.00                            2.09   \n",
       "184                           0.97                            2.14   \n",
       "185                           0.20                            1.37   \n",
       "186                           1.69                            1.75   \n",
       "187                           0.40                            2.16   \n",
       "\n",
       "     Hispanic/Latino  \n",
       "0               2.19  \n",
       "1               4.05  \n",
       "2               1.11  \n",
       "3               5.60  \n",
       "4               2.68  \n",
       "..               ...  \n",
       "183             2.98  \n",
       "184             2.00  \n",
       "185             8.73  \n",
       "186             5.19  \n",
       "187            19.07  \n",
       "\n",
       "[188 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_averages_percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
