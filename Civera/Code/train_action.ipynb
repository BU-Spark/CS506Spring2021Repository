{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statewide-footwear",
   "metadata": {},
   "source": [
    "Branch: master\n",
    "\n",
    "File: train_action.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "active-transportation",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5547d87d5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmysql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mysql'"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from csv import writer\n",
    "import copy\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-ranking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mydb = mysql.connector.connect(host='73.38.248.152', user='buspark', password='U@5p1r3!')\n",
    "\n",
    "if (mydb):\n",
    "    print(\"Connection Successful\")\n",
    "else:\n",
    "    print(\"Connection Unsuccessful\")\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "# Load sql to dataframe \n",
    "# Get Training Set (Action != NULL and Actor != NULL)\n",
    "# Getting 10000 values first \n",
    "case_index_not_null = pd.read_sql(\"SELECT * FROM wp_courtdocs.cdocs_case_action_index as c_a_index WHERE c_a_index.action != ' ' and c_a_index.actor != ' ' LIMIT 10000;\", con = mydb)\n",
    "columns = ['actor','action','description']\n",
    "trainSet = case_index_not_null[columns]\n",
    "print(trainSet.head())\n",
    "\n",
    "# Get Test Set (Action = NULL)\n",
    "# Getting 10000 values first \n",
    "action_null = pd.read_sql(\"SELECT * FROM wp_courtdocs.cdocs_case_action_index as c_a_index WHERE c_a_index.action = ' ' LIMIT 10000;\", con = mydb)\n",
    "testSet = action_null[columns]\n",
    "print(testSet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-arabic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Distinct Values of Actions Field with Index Number \n",
    "path1 = \"C:\\\\Users\\\\Serra\\\\Desktop\\\\CS506Spring2021Repository\\\\Civera\\\\Data\\\\distinct-case-actions.txt\"\n",
    "distinct_actions = pd.read_csv(path1)\n",
    "print(distinct_actions.head())\n",
    "\n",
    "#merge training set with distinct-case-actions.txt to get the index value for distinct actions \n",
    "trainSet = trainSet.merge(distinct_actions, on='action')\n",
    "print(trainSet.head())\n",
    "\n",
    "r = re.compile(r'[^\\w\\s]+')\n",
    "trainSet['description'] = [r.sub('', x) for x in trainSet['description'].tolist()]\n",
    "trainSet['description'] = trainSet['description'].str.lower().str.split()\n",
    "print(trainSet.head())\n",
    "\n",
    "testSet['description'] = [r.sub('', x) for x in testSet['description'].tolist()]\n",
    "testSet['description'] = testSet['description'].str.lower().str.split()\n",
    "print(testSet.head())\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "#remove stopwords in trainSet \n",
    "trainSet['description'] = trainSet['description'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "print(\"stopwords\")\n",
    "print(trainSet.head())\n",
    "print()\n",
    "\n",
    "#remove stopwords in testSet \n",
    "testSet['description'] = testSet['description'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "print(testSet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-dollar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use Lemmatizer for train and test set \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "trainSet['description'] = trainSet['description'].apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\n",
    "testSet['description'] = testSet['description'].apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "#remove duplicate words after lemmatizing \n",
    "trainSet['description'] = trainSet['description'].apply(lambda x:list(dict.fromkeys(x)))\n",
    "print()\n",
    "print('trainingSet after lemmatizer & removing dupes ')\n",
    "print(trainSet.head())\n",
    "\n",
    "testSet['description'] = testSet['description'].apply(lambda x:list(dict.fromkeys(x)))\n",
    "print()\n",
    "print('testSet after lemmatizer & removing dupes ')\n",
    "print(testSet.head())\n",
    "\n",
    "#copy\n",
    "trainSet1 = copy.deepcopy(trainSet)\n",
    "testSet1 = copy.deepcopy(testSet)\n",
    "\n",
    "#join back \n",
    "trainSet1['description'] = trainSet1 ['description'].apply(lambda x:' '.join(x))\n",
    "testSet1['description'] = testSet1['description'].apply(lambda x:' '.join(x))\n",
    "print()\n",
    "\n",
    "trainSet1['description'] = trainSet1['description'].astype('str')\n",
    "testSet1['description'] = testSet1['description'].astype('str')\n",
    "\n",
    "print(\"preprocessing done\")\n",
    "print(trainSet1.head())\n",
    "print(testSet1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-exposure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train-test-split starts \n",
    "X = trainSet1['description']\n",
    "y = trainSet1['action_index']\n",
    "\n",
    "print(\"train-test-split processing\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)\n",
    "\n",
    "clf1 = Pipeline([('tfidf', TfidfVectorizer()),('rdf',RandomForestClassifier()),])\n",
    "# training data through the pipeline\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "clf2 = Pipeline([('tfidf', TfidfVectorizer()),('mnb',MultinomialNB()),])\n",
    "# training data through the pipeline\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "#RandomForest Prediction \n",
    "prediction1 = clf1.predict(testSet1['description'])\n",
    "print(prediction1.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-assessment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MultinomialNB Prediction \n",
    "prediction2 = clf2.predict(testSet1['description'])\n",
    "print(prediction2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-bradford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission1 = pd.DataFrame({'actor':testSet1['actor'],'description':testSet1['description'],'action_index':prediction1})\n",
    "#Visualize the first 5 rows\n",
    "print(\"prediction 1\")\n",
    "print(submission1.head())\n",
    "submission1 = submission1.merge(distinct_actions, on='action_index') \n",
    "\n",
    "submission2 = pd.DataFrame({'actor':testSet1['actor'],'description':testSet1['description'],'action_index':prediction2})\n",
    "print(\"prediction 2\")\n",
    "print(submission2.head())\n",
    "submission2 = submission2.merge(distinct_actions, on='action_index') \n",
    "\n",
    "path4 = 'C:\\\\Users\\\\Serra\\\\Desktop\\\\CS506Spring2021Repository\\\\Civera\\\\Data\\\\RandomForest-Prediction.txt'\n",
    "path5 = 'C:\\\\Users\\\\Serra\\\\Desktop\\\\CS506Spring2021Repository\\\\Civera\\\\Data\\\\MultinomialNB-Prediction.txt'\n",
    "submission1.to_csv(path4, mode='w', index = False)\n",
    "submission2.to_csv(path5, mode='w', index = False)\n",
    "#testSet1.to_csv(path3, mode='w', index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
